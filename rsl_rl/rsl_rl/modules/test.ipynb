{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import escnn\n",
    "import escnn.group\n",
    "from escnn.group import CyclicGroup, DihedralGroup, DirectProductGroup, Group, Representation\n",
    "from escnn.nn import FieldType, EquivariantModule, GeometricTensor\n",
    "\n",
    "from morpho_symm.utils.algebra_utils import gen_permutation_matrix\n",
    "from morpho_symm.utils.rep_theory_utils import group_rep_from_gens\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# symmetry_space = escnn.gspaces.GSpace3D(tuple([False, False, 2]))\n",
    "# G = symmetry_space.fibergroup\n",
    "np.set_printoptions(precision=1, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_repr_to_gspace(G:escnn.group.Group, \n",
    "                       permutation_conf:Union[List[int], np.ndarray],\n",
    "                       reflection_conf:Union[List[int], np.ndarray],\n",
    "                       name:str\n",
    "    ):\n",
    "    \"\"\"将指定的变换添加到G这个group中, 供之后生成escnn的对称网络使用\n",
    "\n",
    "    Args:\n",
    "        G (escnn.group.Group): 一个escnn的group\n",
    "        permutation_conf (Union[List[int], np.ndarray]): 位置变换矩阵\n",
    "        reflection_conf (Union[List[int], np.ndarray]): 由1和-1指定的变换矩阵\n",
    "        name (str): 这个变换的名字\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # 获取对应的配置文件，保证格式为 (n, 配置长度)\n",
    "    permutation_conf = np.array(permutation_conf, dtype=int)\n",
    "    reflection_conf = np.array(reflection_conf, dtype=float)\n",
    "    if permutation_conf.ndim == 1: permutation_conf = permutation_conf[None]\n",
    "    if reflection_conf.ndim == 1: reflection_conf = reflection_conf[None]\n",
    "    \n",
    "    # 获取配置个数, 长度\n",
    "    (conf_num, conf_length) = permutation_conf.shape\n",
    "    print(f\"Conf name = {name}, length = {conf_length}\")\n",
    "    \n",
    "    # 检查: 确保给的配置是正常的\n",
    "    assert permutation_conf.shape == reflection_conf.shape, len(reflection_conf.shape)==2\n",
    "    assert conf_num == len(G.generators)\n",
    "    \n",
    "    # 开始配置representations\n",
    "    rep_joints = {G.identity: np.eye(conf_length, dtype=float)}\n",
    "    for g_gen, perm, refx in zip(G.generators, permutation_conf, reflection_conf):\n",
    "        refx = np.array(refx, dtype=float)\n",
    "        rep_joints[g_gen] = gen_permutation_matrix(oneline_notation=perm, reflections=refx)\n",
    "\n",
    "    # #将dict转化为representation.Representation\n",
    "    rep_joints = group_rep_from_gens(G, rep_joints) \n",
    "    # 配置name\n",
    "    rep_joints.name = name\n",
    "    # 输入给G\n",
    "    G.representations.update(**{name:rep_joints})\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conf name = t1, length = 3\n",
      "Conf name = t2, length = 3\n",
      "Conf name = tri, length = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = CyclicGroup(2)\n",
    "gspace = escnn.gspaces.no_base_space(G)\n",
    "\n",
    "add_repr_to_gspace(G, [2,1,0], [-1,1,-1], 't1')\n",
    "add_repr_to_gspace(G, [1,0,2], [1,1,-1], 't2')\n",
    "\n",
    "add_repr_to_gspace(G, [0], [1], 'tri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'irrep_0': C2|[irrep_0]:1,\n",
       " 'irrep_1': C2|[irrep_1]:1,\n",
       " 'regular': C2|[regular]:2,\n",
       " 't1': C2|[t1]:3,\n",
       " 't2': C2|[t2]:3,\n",
       " 'tri': C2|[tri]:1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.representations['tri'](G.elements[1])  # 这个就是 trivial_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.trivial_representation(G.elements[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEMLP(EquivariantModule):\n",
    "    def __init__(self,\n",
    "                 in_type: FieldType,\n",
    "                 out_type: FieldType,\n",
    "                 hidden_dims = [256, 256, 256],\n",
    "                 bias: bool = True,\n",
    "                 actor: bool = True,\n",
    "                 activation: str = \"ReLU\"):\n",
    "        super().__init__()\n",
    "        self.in_type = in_type\n",
    "        self.out_type = out_type\n",
    "        gspace = in_type.gspace\n",
    "        group = gspace.fibergroup\n",
    "        \n",
    "        layer_in_type = in_type\n",
    "        self.net = escnn.nn.SequentialModule()\n",
    "        for n in range(len(hidden_dims)):\n",
    "            layer_out_type = FieldType(gspace, [group.regular_representation] * int((hidden_dims[n] / group.order())))\n",
    "\n",
    "            self.net.add_module(f\"linear_{n}: in={layer_in_type.size}-out={layer_out_type.size}\",\n",
    "                             escnn.nn.Linear(layer_in_type, layer_out_type, bias=bias))\n",
    "            self.net.add_module(f\"act_{n}\", self.get_activation(activation, layer_out_type))\n",
    "\n",
    "            layer_in_type = layer_out_type\n",
    "\n",
    "        if actor: \n",
    "            self.net.add_module(f\"linear_{len(hidden_dims)}: in={layer_in_type.size}-out={out_type.size}\",\n",
    "                                escnn.nn.Linear(layer_in_type, out_type, bias=bias))\n",
    "            self.extra_layer = None\n",
    "        else:\n",
    "            num_inv_features = len(layer_in_type.irreps)\n",
    "            self.extra_layer = torch.nn.Linear(num_inv_features, out_type.size, bias=False)\n",
    "\n",
    "    def forward(self, x: GeometricTensor) -> GeometricTensor:\n",
    "        x= self.net(x)\n",
    "        if self.extra_layer:\n",
    "            x = self.extra_layer(x.tensor)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def get_activation(activation: str, hidden_type: FieldType) -> EquivariantModule:\n",
    "        if activation.lower() == \"relu\":\n",
    "            return escnn.nn.ReLU(hidden_type)\n",
    "        elif activation.lower() == \"elu\":\n",
    "            return escnn.nn.ELU(hidden_type)\n",
    "        elif activation.lower() == \"lrelu\":\n",
    "            return escnn.nn.LeakyReLU(hidden_type)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def evaluate_output_shape(self, input_shape):\n",
    "        \"\"\"Returns the output shape of the model given an input shape.\"\"\"\n",
    "        batch_size = input_shape[0]\n",
    "        return batch_size, self.out_type.size\n",
    "\n",
    "    def export(self):\n",
    "        \"\"\"Exports the model to a torch.nn.Sequential instance.\"\"\"\n",
    "        sequential = nn.Sequential()\n",
    "        for name, module in self.net.named_children():\n",
    "            sequential.add_module(name, module.export())\n",
    "        return sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_input_transitions = [G.representations['t1'],\n",
    "                           G.representations['t2']]\n",
    "actor_output_transitions = [G.representations['t1']]\n",
    "\n",
    "in_field_type = FieldType(gspace, actor_input_transitions)\n",
    "out_field_type = FieldType(gspace, actor_output_transitions)\n",
    "\n",
    "actor = SimpleEMLP(in_field_type, out_field_type,\n",
    "            hidden_dims = [256, 256, 256], \n",
    "            activation = 'elu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "observations = np.arange(12, dtype=np.float32).reshape(2,6)\n",
    "obs_torch = torch.as_tensor(observations, device=device)\n",
    "actor = actor.to(device=device)\n",
    "\n",
    "res1 = actor(in_field_type(obs_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.,  4.,  5.],\n",
       "       [ 6.,  7.,  8.,  9., 10., 11.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g_tensor([[  0.2529,  -4.7182,  -1.1217],\n",
       "          [  2.6172, -13.7864,  -3.6580]], device='cuda:0',\n",
       "         grad_fn=<AddmmBackward0>, [C2: {t1 (x1)}(3)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symm_tensor(data:torch.Tensor, G:escnn.group.Group, reprs:List[str])->torch.Tensor:\n",
    "    \"\"\"将data这个torch.tensor转化为对称后的结构\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): (Batch, N)\n",
    "        G (escnn.group.Group): 一个escnn的群论group\n",
    "        reprs (List): represetations的列表，会按照顺序对data进行对称计算\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: 返回的是对称后的结果，与data保持相同的device\n",
    "    \"\"\"\n",
    "    # 要求data数据是torch.tensor\n",
    "    assert isinstance(data, torch.Tensor), data.ndim <= 2\n",
    "    # 整理data的shape和 repr要是列表\n",
    "    data = data[None] if data.ndim == 1 else data\n",
    "    reprs = [reprs] if not isinstance(reprs, List) else reprs\n",
    "    # 获取device\n",
    "    device = data.device\n",
    "    # 开始转换\n",
    "    curr_ind = 0\n",
    "    res = []\n",
    "    for repr in reprs:\n",
    "        res.append(\n",
    "            (torch.as_tensor(G.representations[repr](G.elements[1]), dtype=torch.float32, device=device) \\\n",
    "                @ data.T[curr_ind:curr_ind+G.representations[repr].size]\n",
    "                ).T \n",
    "        )\n",
    "        curr_ind += G.representations[repr].size\n",
    "    # 确保reprs和data的维度是吻合的\n",
    "    assert curr_ind == data.shape[-1]\n",
    "    # 返回结果\n",
    "    return torch.concat(res, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -2,   1,   0,   4,   3,  -5],\n",
      "        [ -8,   7,  -6,  10,   9, -11]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "o2 = get_symm_tensor(obs_torch, G, ['t1', 't2'])\n",
    "print(o2.int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_tensor([[  1.1217,  -4.7182,  -0.2529],\n",
      "          [  3.6580, -13.7865,  -2.6172]], device='cuda:0',\n",
      "         grad_fn=<AddmmBackward0>, [C2: {t1 (x1)}(3)])\n"
     ]
    }
   ],
   "source": [
    "res2 = actor(in_field_type(o2))\n",
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# a = np.array([-0.0001, 1, -2,\\\n",
    "#         3, -4, 5,\\\n",
    "#         6, -7, -8,])\n",
    "\n",
    "# def transform_zycofig_to_code(config:list):\n",
    "#     a = np.array(config)\n",
    "#     tmp = np.abs(a).astype(int)\n",
    "#     permutation = tmp - tmp.min()\n",
    "#     is_negative = np.where(a>0, 1, -1)\n",
    "#     return permutation.tolist(), is_negative.tolist()\n",
    "    \n",
    "# def transform_all(*configs):\n",
    "#     for i,config in enumerate(configs):\n",
    "#         permutation, is_negative = transform_zycofig_to_code(config)\n",
    "#         print(f\"i:{i}, length:{len(permutation)}\")\n",
    "#         print(permutation)\n",
    "#         print(is_negative)\n",
    "#         print()\n",
    "\n",
    "# transform_all([\n",
    "#     -0.0001, 1, -2,\\\n",
    "#     ],[\n",
    "#     3, -4, 5,\\\n",
    "#     ],[    \n",
    "#     6, -7, -8,\\\n",
    "#     ],[\n",
    "#     15, -16, -17, 18, 19, -20,\\\n",
    "#     9, -10, -11, 12, 13, -14,\\\n",
    "#     -21,\\\n",
    "#     29, -30, -31, 32, -33, 34, -35,\\\n",
    "#     22, -23, -24, 25, -26, 27, -28,\\\n",
    "#     ],[\n",
    "#     42, -43, -44, 45, 46, -47,\\\n",
    "#     36, -37, -38, 39, 40, -41,\\\n",
    "#     -48,\\\n",
    "#     56, -57, -58, 59, -60, 61, -62,\\\n",
    "#     49, -50, -51, 52, -53, 54, -55,\\\n",
    "#     ],[\n",
    "#     69, -70, -71, 72, 73, -74,\\\n",
    "#     63, -64, -65, 66, 67, -68,\\\n",
    "#     -75,\\\n",
    "#     83, -84, -85, 86, -87, 88, -89,\\\n",
    "#     76, -77, -78, 79, -80, 81, -82,\n",
    "#     ],[\n",
    "#     -90, -91 \n",
    "#                           ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "1[2pi/2]\n",
      "1[2pi/2] 2.3841858e-07 8.443991e-08 4.8356383e-15\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n",
      "0[2pi/2]\n",
      "0[2pi/2] 0.0 0.0 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (0[2pi/2], 0.0),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (0[2pi/2], 0.0),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (1[2pi/2], 8.443991e-08),\n",
       " (0[2pi/2], 0.0),\n",
       " (0[2pi/2], 0.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 = SimpleEMLP(in_field_type, out_field_type,\n",
    "            hidden_dims = [256, 256, 256], \n",
    "            activation = 'elu')\n",
    "a2 = a2.to('cpu')\n",
    "a2.check_equivariance(1e-5, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleEMLP(\n",
      "  (net): SequentialModule(\n",
      "    (linear_0: in=6-out=256): Linear(\n",
      "      (_basisexpansion): BlocksBasisExpansion(\n",
      "        (block_expansion_('t1', 'regular')): SingleBlockBasisExpansion()\n",
      "        (block_expansion_('t2', 'regular')): SingleBlockBasisExpansion()\n",
      "      )\n",
      "    )\n",
      "    (act_0): ELU(alpha=1.0, inplace=False, type=[C2: {regular (x128)}(256)])\n",
      "    (linear_1: in=256-out=256): Linear(\n",
      "      (_basisexpansion): BlocksBasisExpansion(\n",
      "        (block_expansion_('regular', 'regular')): SingleBlockBasisExpansion()\n",
      "      )\n",
      "    )\n",
      "    (act_1): ELU(alpha=1.0, inplace=False, type=[C2: {regular (x128)}(256)])\n",
      "    (linear_2: in=256-out=256): Linear(\n",
      "      (_basisexpansion): BlocksBasisExpansion(\n",
      "        (block_expansion_('regular', 'regular')): SingleBlockBasisExpansion()\n",
      "      )\n",
      "    )\n",
      "    (act_2): ELU(alpha=1.0, inplace=False, type=[C2: {regular (x128)}(256)])\n",
      "    (linear_3: in=256-out=3): Linear(\n",
      "      (_basisexpansion): BlocksBasisExpansion(\n",
      "        (block_expansion_('regular', 't1')): SingleBlockBasisExpansion()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['net.linear_0: in=6-out=256.bias',\n",
       " 'net.linear_0: in=6-out=256.weights',\n",
       " 'net.linear_0: in=6-out=256.bias_expansion',\n",
       " 'net.linear_0: in=6-out=256.expanded_bias',\n",
       " 'net.linear_0: in=6-out=256.matrix',\n",
       " \"net.linear_0: in=6-out=256._basisexpansion.block_expansion_('t1', 'regular').sampled_basis\",\n",
       " \"net.linear_0: in=6-out=256._basisexpansion.block_expansion_('t2', 'regular').sampled_basis\",\n",
       " 'net.linear_1: in=256-out=256.bias',\n",
       " 'net.linear_1: in=256-out=256.weights',\n",
       " 'net.linear_1: in=256-out=256.bias_expansion',\n",
       " 'net.linear_1: in=256-out=256.expanded_bias',\n",
       " 'net.linear_1: in=256-out=256.matrix',\n",
       " \"net.linear_1: in=256-out=256._basisexpansion.block_expansion_('regular', 'regular').sampled_basis\",\n",
       " 'net.linear_2: in=256-out=256.bias',\n",
       " 'net.linear_2: in=256-out=256.weights',\n",
       " 'net.linear_2: in=256-out=256.bias_expansion',\n",
       " 'net.linear_2: in=256-out=256.expanded_bias',\n",
       " 'net.linear_2: in=256-out=256.matrix',\n",
       " \"net.linear_2: in=256-out=256._basisexpansion.block_expansion_('regular', 'regular').sampled_basis\",\n",
       " 'net.linear_3: in=256-out=3.bias',\n",
       " 'net.linear_3: in=256-out=3.weights',\n",
       " 'net.linear_3: in=256-out=3.bias_expansion',\n",
       " 'net.linear_3: in=256-out=3.expanded_bias',\n",
       " 'net.linear_3: in=256-out=3.matrix',\n",
       " \"net.linear_3: in=256-out=3._basisexpansion.block_expansion_('regular', 't1').sampled_basis\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a2.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.load_state_dict(a2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
